{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel '.venv (Python 3.10.12)' due to a timeout waiting for the ports to get used. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.utils import save_image\n",
    "from models.gpt import GPT, GPTConfig\n",
    "from jsonargparse import ArgumentParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working with z of shape (1, 256, 16, 16) = 65536 dimensions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/ndp/.venv/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/mnt/ndp/.venv/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded pretrained LPIPS loss from taming/modules/autoencoder/lpips/vgg.pth\n",
      "VQLPIPSWithDiscriminator running with hinge loss.\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "# Load YAML file\n",
    "with open('configs/gpt/imagenet_64_12.yaml', 'r') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "model_config_args = config['model']['init_args']['config']['init_args']\n",
    "gpt_config = GPTConfig(**model_config_args)\n",
    "tokenizer_config_args = config['model']['init_args']['tokenizer']['init_args']\n",
    "tokenizer_class = config['model']['init_args']['tokenizer']['class_path']\n",
    "module_path, class_name = tokenizer_class.rsplit('.', 1)\n",
    "module = __import__(module_path, fromlist=[class_name])\n",
    "TokenizerClass = getattr(module, class_name)\n",
    "# Instantiate the tokenizer with the config arguments\n",
    "tokenizer = TokenizerClass(**tokenizer_config_args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 216.88M\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (transformer): OptimizedModule(\n",
       "    (_orig_mod): ModuleDict(\n",
       "      (wce): Embedding(1001, 768)\n",
       "      (wte): Embedding(16384, 768)\n",
       "      (wpe): Embedding(342, 768)\n",
       "      (drop): Dropout(p=0.05, inplace=False)\n",
       "      (h): ModuleList(\n",
       "        (0-11): 12 x Block(\n",
       "          (ln_1): LayerNorm()\n",
       "          (attn): CausalSelfAttention(\n",
       "            (c_attn): Linear(in_features=768, out_features=2304, bias=False)\n",
       "            (c_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (attn_dropout): Dropout(p=0.05, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.05, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm()\n",
       "          (mlp): MLP(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=False)\n",
       "            (gelu): GELU(approximate='none')\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=False)\n",
       "            (dropout): Dropout(p=0.05, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (ln_f): LayerNorm()\n",
       "    )\n",
       "  )\n",
       "  (content_head): Linear(in_features=768, out_features=16384, bias=False)\n",
       "  (position_head): Linear(in_features=768, out_features=342, bias=False)\n",
       "  (tokenizer): OptimizedModule(\n",
       "    (_orig_mod): IBQSharedModel(\n",
       "      (encoder_1): Encoder(\n",
       "        (conv_in): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (down): ModuleList(\n",
       "          (0): Module(\n",
       "            (block): ModuleList(\n",
       "              (0-1): 2 x ResnetBlock(\n",
       "                (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "                (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "                (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              )\n",
       "            )\n",
       "            (attn): ModuleList()\n",
       "            (downsample): Downsample(\n",
       "              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2))\n",
       "            )\n",
       "          )\n",
       "          (1): Module(\n",
       "            (block): ModuleList(\n",
       "              (0): ResnetBlock(\n",
       "                (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "                (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "                (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (nin_shortcut): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "              )\n",
       "              (1): ResnetBlock(\n",
       "                (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "                (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "                (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              )\n",
       "            )\n",
       "            (attn): ModuleList()\n",
       "            (downsample): Downsample(\n",
       "              (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))\n",
       "            )\n",
       "          )\n",
       "          (2): Module(\n",
       "            (block): ModuleList(\n",
       "              (0): ResnetBlock(\n",
       "                (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "                (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "                (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (nin_shortcut): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "              )\n",
       "              (1): ResnetBlock(\n",
       "                (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "                (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "                (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              )\n",
       "            )\n",
       "            (attn): ModuleList(\n",
       "              (0-1): 2 x AttnBlock(\n",
       "                (norm): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "                (q): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (k): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (v): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (proj_out): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (mid): Module(\n",
       "          (block_1): ResnetBlock(\n",
       "            (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (attn_1): AttnBlock(\n",
       "            (norm): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (q): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (k): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (v): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (proj_out): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (block_2): ResnetBlock(\n",
       "            (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (norm_out): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "        (conv_out): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (encoder_2): Encoder(\n",
       "        (conv_in): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (down): ModuleList(\n",
       "          (0): Module(\n",
       "            (block): ModuleList(\n",
       "              (0-1): 2 x ResnetBlock(\n",
       "                (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "                (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "                (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              )\n",
       "            )\n",
       "            (attn): ModuleList()\n",
       "            (downsample): Downsample(\n",
       "              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2))\n",
       "            )\n",
       "          )\n",
       "          (1): Module(\n",
       "            (block): ModuleList(\n",
       "              (0): ResnetBlock(\n",
       "                (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "                (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "                (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (nin_shortcut): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "              )\n",
       "              (1): ResnetBlock(\n",
       "                (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "                (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "                (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              )\n",
       "            )\n",
       "            (attn): ModuleList(\n",
       "              (0-1): 2 x AttnBlock(\n",
       "                (norm): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "                (q): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (k): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (v): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (proj_out): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (mid): Module(\n",
       "          (block_1): ResnetBlock(\n",
       "            (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (attn_1): AttnBlock(\n",
       "            (norm): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (q): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (k): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (v): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (proj_out): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (block_2): ResnetBlock(\n",
       "            (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (norm_out): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "        (conv_out): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (encoder_3): Encoder(\n",
       "        (conv_in): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (down): ModuleList(\n",
       "          (0): Module(\n",
       "            (block): ModuleList(\n",
       "              (0-1): 2 x ResnetBlock(\n",
       "                (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "                (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "                (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              )\n",
       "            )\n",
       "            (attn): ModuleList()\n",
       "            (downsample): Downsample(\n",
       "              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2))\n",
       "            )\n",
       "          )\n",
       "          (1): Module(\n",
       "            (block): ModuleList(\n",
       "              (0): ResnetBlock(\n",
       "                (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "                (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "                (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (nin_shortcut): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "              )\n",
       "              (1): ResnetBlock(\n",
       "                (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "                (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "                (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              )\n",
       "            )\n",
       "            (attn): ModuleList(\n",
       "              (0-1): 2 x AttnBlock(\n",
       "                (norm): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "                (q): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (k): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (v): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (proj_out): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (mid): Module(\n",
       "          (block_1): ResnetBlock(\n",
       "            (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (attn_1): AttnBlock(\n",
       "            (norm): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (q): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (k): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (v): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (proj_out): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (block_2): ResnetBlock(\n",
       "            (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (norm_out): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "        (conv_out): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (encoder_4): Encoder(\n",
       "        (conv_in): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (down): ModuleList(\n",
       "          (0): Module(\n",
       "            (block): ModuleList(\n",
       "              (0-1): 2 x ResnetBlock(\n",
       "                (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "                (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "                (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              )\n",
       "            )\n",
       "            (attn): ModuleList()\n",
       "            (downsample): Downsample(\n",
       "              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2))\n",
       "            )\n",
       "          )\n",
       "          (1): Module(\n",
       "            (block): ModuleList(\n",
       "              (0): ResnetBlock(\n",
       "                (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "                (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "                (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (nin_shortcut): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "              )\n",
       "              (1): ResnetBlock(\n",
       "                (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "                (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "                (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              )\n",
       "            )\n",
       "            (attn): ModuleList(\n",
       "              (0-1): 2 x AttnBlock(\n",
       "                (norm): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "                (q): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (k): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (v): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (proj_out): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (mid): Module(\n",
       "          (block_1): ResnetBlock(\n",
       "            (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (attn_1): AttnBlock(\n",
       "            (norm): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (q): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (k): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (v): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (proj_out): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (block_2): ResnetBlock(\n",
       "            (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (norm_out): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "        (conv_out): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (decoder): Decoder(\n",
       "        (conv_in): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (mid): Module(\n",
       "          (block_1): ResnetBlock(\n",
       "            (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (attn_1): AttnBlock(\n",
       "            (norm): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (q): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (k): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (v): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (proj_out): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (block_2): ResnetBlock(\n",
       "            (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (up): ModuleList(\n",
       "          (0): Module(\n",
       "            (block): ModuleList(\n",
       "              (0): ResnetBlock(\n",
       "                (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "                (conv1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "                (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (nin_shortcut): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "              )\n",
       "              (1-2): 2 x ResnetBlock(\n",
       "                (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "                (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "                (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              )\n",
       "            )\n",
       "            (attn): ModuleList()\n",
       "          )\n",
       "          (1): Module(\n",
       "            (block): ModuleList(\n",
       "              (0): ResnetBlock(\n",
       "                (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "                (conv1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "                (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (nin_shortcut): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "              )\n",
       "              (1-2): 2 x ResnetBlock(\n",
       "                (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "                (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "                (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              )\n",
       "            )\n",
       "            (attn): ModuleList()\n",
       "            (upsample): Upsample(\n",
       "              (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "          )\n",
       "          (2): Module(\n",
       "            (block): ModuleList(\n",
       "              (0-2): 3 x ResnetBlock(\n",
       "                (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "                (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "                (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              )\n",
       "            )\n",
       "            (attn): ModuleList(\n",
       "              (0-2): 3 x AttnBlock(\n",
       "                (norm): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "                (q): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (k): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (v): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (proj_out): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "              )\n",
       "            )\n",
       "            (upsample): Upsample(\n",
       "              (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (norm_out): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "        (conv_out): Conv2d(128, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (quantize): IndexPropagationQuantize(\n",
       "        (embedding): Embedding(16384, 256)\n",
       "      )\n",
       "      (quant_conv_1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (quant_conv_2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (quant_conv_3): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (quant_conv_4): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (post_quant_conv_1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (post_quant_conv_2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (post_quant_conv_3): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (post_quant_conv_4): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (loss): VQLPIPSWithDiscriminator(\n",
       "        (perceptual_loss): LPIPS(\n",
       "          (scaling_layer): ScalingLayer()\n",
       "          (net): vgg16(\n",
       "            (slice1): Sequential(\n",
       "              (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (3): ReLU(inplace=True)\n",
       "            )\n",
       "            (slice2): Sequential(\n",
       "              (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "              (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (6): ReLU(inplace=True)\n",
       "              (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (8): ReLU(inplace=True)\n",
       "            )\n",
       "            (slice3): Sequential(\n",
       "              (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "              (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (11): ReLU(inplace=True)\n",
       "              (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (13): ReLU(inplace=True)\n",
       "              (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (15): ReLU(inplace=True)\n",
       "            )\n",
       "            (slice4): Sequential(\n",
       "              (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "              (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (18): ReLU(inplace=True)\n",
       "              (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (20): ReLU(inplace=True)\n",
       "              (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (22): ReLU(inplace=True)\n",
       "            )\n",
       "            (slice5): Sequential(\n",
       "              (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "              (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (25): ReLU(inplace=True)\n",
       "              (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (27): ReLU(inplace=True)\n",
       "              (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (29): ReLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "          (lin0): NetLinLayer(\n",
       "            (model): Sequential(\n",
       "              (0): Dropout(p=0.5, inplace=False)\n",
       "              (1): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            )\n",
       "          )\n",
       "          (lin1): NetLinLayer(\n",
       "            (model): Sequential(\n",
       "              (0): Dropout(p=0.5, inplace=False)\n",
       "              (1): Conv2d(128, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            )\n",
       "          )\n",
       "          (lin2): NetLinLayer(\n",
       "            (model): Sequential(\n",
       "              (0): Dropout(p=0.5, inplace=False)\n",
       "              (1): Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            )\n",
       "          )\n",
       "          (lin3): NetLinLayer(\n",
       "            (model): Sequential(\n",
       "              (0): Dropout(p=0.5, inplace=False)\n",
       "              (1): Conv2d(512, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            )\n",
       "          )\n",
       "          (lin4): NetLinLayer(\n",
       "            (model): Sequential(\n",
       "              (0): Dropout(p=0.5, inplace=False)\n",
       "              (1): Conv2d(512, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (discriminator): NLayerDiscriminator(\n",
       "          (main): Sequential(\n",
       "            (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "            (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "            (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "            (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "            (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "            (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "            (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (10): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "            (11): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GPT.load_from_checkpoint(\"imagenet_64/gpt/epoch=299-step=375600.ckpt\", config=gpt_config, tokenizer=tokenizer)\n",
    "model.eval()\n",
    "model.half()\n",
    "model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/340 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content_idx_cond torch.Size([1, 1]) torch.int64\n",
      "position_idx_cond torch.Size([1, 1]) torch.int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (12x64 and 768x768)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m z1, z2, z3, z4 \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mget_zero_tokens(\u001b[38;5;241m1\u001b[39m, tokenizer\u001b[38;5;241m.\u001b[39membed_dim, model\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 8\u001b[0m     content_tokens, positions \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpositions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m340\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.25\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcfg_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m ind4 \u001b[38;5;241m=\u001b[39m content_tokens[:, \u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m5\u001b[39m]\n\u001b[1;32m     11\u001b[0m ind3 \u001b[38;5;241m=\u001b[39m content_tokens[:, \u001b[38;5;241m5\u001b[39m:\u001b[38;5;241m21\u001b[39m]\n",
      "File \u001b[0;32m/mnt/ndp/.venv/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/ndp/models/gpt.py:324\u001b[0m, in \u001b[0;36mGPT.generate\u001b[0;34m(self, labels, positions, max_new_tokens, temperature, top_k, cfg_scale)\u001b[0m\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent_idx_cond\u001b[39m\u001b[38;5;124m'\u001b[39m, content_idx_cond\u001b[38;5;241m.\u001b[39mshape, content_idx_cond\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mposition_idx_cond\u001b[39m\u001b[38;5;124m'\u001b[39m, position_idx_cond\u001b[38;5;241m.\u001b[39mshape, position_idx_cond\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[0;32m--> 324\u001b[0m     content_logits, position_logits, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent_idx_cond\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposition_idx_cond\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    326\u001b[0m     content_idx \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((content_idx_cond, content_idx_uncond), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m/mnt/ndp/models/gpt.py:201\u001b[0m, in \u001b[0;36mGPT.forward\u001b[0;34m(self, content_tokens, position_tokens, content_targets, position_targets)\u001b[0m\n\u001b[1;32m    199\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer\u001b[38;5;241m.\u001b[39mdrop(tok_emb \u001b[38;5;241m+\u001b[39m pos_emb)\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer\u001b[38;5;241m.\u001b[39mh:\n\u001b[0;32m--> 201\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    202\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer\u001b[38;5;241m.\u001b[39mln_f(x)\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m content_targets \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m position_targets \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;66;03m# if we are given some desired targets also calculate the loss\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/ndp/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1749\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1748\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/ndp/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1760\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1755\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1756\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1758\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1759\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1760\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1762\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1763\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/mnt/ndp/models/gpt.py:92\u001b[0m, in \u001b[0;36mBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 92\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln_1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_2(x))\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/mnt/ndp/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1749\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1748\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/ndp/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1760\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1755\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1756\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1758\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1759\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1760\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1762\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1763\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/mnt/ndp/models/gpt.py:63\u001b[0m, in \u001b[0;36mCausalSelfAttention.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     61\u001b[0m y \u001b[38;5;241m=\u001b[39m flash_attn_func(q, k, v, dropout_p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0.0\u001b[39m, causal\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# output projection\u001b[39;00m\n\u001b[0;32m---> 63\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresid_dropout(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m y\n",
      "File \u001b[0;32m/mnt/ndp/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1749\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1748\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/ndp/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1760\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1755\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1756\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1758\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1759\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1760\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1762\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1763\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/mnt/ndp/.venv/lib/python3.10/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (12x64 and 768x768)"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "import numpy as np\n",
    "\n",
    "labels = torch.tensor([[75]], dtype=torch.long, device=model.device)\n",
    "positions = torch.tensor([[0]], dtype=torch.long, device=model.device)\n",
    "z1, z2, z3, z4 = tokenizer.get_zero_tokens(1, tokenizer.embed_dim, model.device)\n",
    "with torch.no_grad():\n",
    "    content_tokens, positions = model.generate(labels, positions, max_new_tokens=340, temperature=1.25, cfg_scale=1.0)\n",
    "    \n",
    "ind4 = content_tokens[:, 1:5]\n",
    "ind3 = content_tokens[:, 5:21]\n",
    "ind2 = content_tokens[:, 21:85]\n",
    "ind1 = content_tokens[:, 85:]\n",
    "\n",
    "q1 = tokenizer.quantize.get_codebook_entry(ind1, shape=(1, 16, 16, tokenizer.embed_dim))\n",
    "q2 = tokenizer.quantize.get_codebook_entry(ind2, shape=(1, 8, 8, tokenizer.embed_dim))\n",
    "q3 = tokenizer.quantize.get_codebook_entry(ind3, shape=(1, 4, 4, tokenizer.embed_dim))\n",
    "q4 = tokenizer.quantize.get_codebook_entry(ind4, shape=(1, 2, 2, tokenizer.embed_dim))\n",
    "\n",
    "images = [tokenizer.decode(z1, z2, z3, q4), tokenizer.decode(z1, z2, q3, q4), tokenizer.decode(z1, q2, q3, q4), tokenizer.decode(q1, q2, q3, q4)]\n",
    "images = torch.cat((images[0], images[1], images[2], images[3])).detach().cpu()\n",
    "images = torch.clamp(images, -1., 1.)\n",
    "\n",
    "grid = torchvision.utils.make_grid(images, nrow=4)\n",
    "grid = (grid+1.0)/2.0 # -1,1 -> 0,1; c,h,w\n",
    "grid = grid.transpose(0,1).transpose(1,2).squeeze(-1)\n",
    "grid = grid.numpy()\n",
    "grid = (grid*255).astype(np.uint8)\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "# Convert numpy array to PIL Image\n",
    "img = Image.fromarray(grid)\n",
    "# Display the image in the notebook\n",
    "plt.figure(figsize=(12, 3))\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
