model:
  class_path: models.gpt.GPT
  init_args:
    config:
      class_path: models.gpt.GPTConfig
      init_args:
        context_length: 1024
        content_vocab_size: 16384
        position_vocab_size: 341
        n_layer: 12
        n_head: 12
        n_embd: 768
        n_classes: 1000
        dropout: 0.0
        weight_decay: 0.0
        learning_rate: 1e-4
        betas: [0.9, 0.999]
        tokenizer:
          class_path: models.gpt.IBQSharedModel
          init_args:
            n_embed: 16384
            embed_dim: 256
            ckpt_path: ./imagenet_64/vqgan_ibq/latest.ckpt
data:
  class_path: train.ImageNet64DataModule
  init_args:
    batch_size: 4
    num_workers: 0

trainer:
  callbacks:
    - class_path: lightning.pytorch.callbacks.ModelCheckpoint
      init_args:
        save_top_k: -1
        every_n_epochs: 1
        dirpath: ./imagenet_64/gpt
    - class_path: lightning.pytorch.callbacks.LearningRateMonitor
      init_args:
        logging_interval: step
    - class_path: train.ImageLogger
      init_args:
        batch_frequency: 10000
        max_images: 4
        clamp: true
        increase_log_steps: true
        log_dir: ./imagenet_64/gpt_logs
  default_root_dir: ./imagenet_64/gpt
  # strategy:
  #   class_path: lightning.pytorch.strategies.DDPStrategy
  #   init_args:
  #     find_unused_parameters: true
  max_epochs: 50
  precision: bf16-mixed
  devices: 1
  # logger:
  #   class_path: lightning.pytorch.loggers.WandbLogger
  #   init_args:
  #     project: diff_gpt
  log_every_n_steps: 10
  accelerator: mps

seed_everything: 23
